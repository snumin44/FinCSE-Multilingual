# ğŸŠ FinCSE-Multilingual
- **Multilingual Financial SimCSE** for searching financial/economical sentences.
- Fine-tuned on financial sentence pairs in six languages (KO, EN, ZN, JA, VI, ID), while remaining functional for other pre-trained languages.
- Reference : [SimCSE Paper](https://aclanthology.org/2021.emnlp-main.552/), [Original Code](https://github.com/princeton-nlp/SimCSE)


## 1. Model
- The FinCSEs are built upon multilingual PLMs, such as **mBERT** and **XLM-RoBERTa**.
- The FinCSE code in this repository is not derived from the original SimCSE code but is newly implemented.

<img src="simcse.PNG" alt="example image" width="600" height="200"/>

## 2. Training Data
- The FinCSEs are fine-tuned using the '[Financial Domain Multilingual Parallel Corpus (ê¸ˆìœµ ë¶„ì•¼ ë‹¤êµ­ì–´ ë³‘ë ¬ ë§ë­‰ì¹˜ ë°ì´í„°)](https://aihub.or.kr/aihubdata/data/view.do?currMenu=&topMenu=&aihubDataSe=data&dataSetSn=71782)' provided by AI HUB in South Korea.
- In this corpus, approximately **2.5 million English, Chinese, Japanese, Vietnamese**, and **Indonesian** sentences in the financial domain are paired with their corresponding **Korean** sentences.
```
(KO) ì´ì²˜ëŸ¼ ê¸ˆìœµìƒí’ˆì˜ ê²½ìš° íŒë§¤ë‹¨ê³„ì—ì„œ ê¸ˆìœµíšŒì‚¬ì˜ ... ìƒí’ˆì˜ ê¶Œìœ ëŠ” ê¸°ë³¸ì´ê³  í•„ìˆ˜ë¼ í•  ê²ƒì´ë‹¤.
(ZN) åƒè¿™æ ·ï¼Œé‡‘èå•†å“åœ¨é”€å”®é˜¶æ®µï¼Œæä¾›é‡‘èå…¬å¸é€‚å½“çš„ä¿¡æ¯å’Œæ¨èé€‚åˆé‡‘èæ¶ˆè´¹è€…çš„å•†å“æ˜¯åŸºæœ¬ï¼Œä¹Ÿæ˜¯å¿…é¡»çš„.
```
- Since this corpus doesn't have similarity scores like a general STS dataset, performance evaluation is based on a **sentence retrieval** task, which searches for corresponding sentences in other languages.

&nbsp;&nbsp; (â€» Note that this corpus is authorized only for individuals with South Korean citizenship.)


## 3. Implementation
You can fine-tune your FinCSE model using your own parallel sentence data.

**(1) Preparing Dataset**
- The dataset must be placed in the **'data'** directory and organized in the following structure.
- The headers and the following parallel sentences must be delimited with '\t' (tab-separated).
```
sent0  sent1
(...)  (...)
```

**(2) Fine-tuning**
- Fine-tune your FinCSE by executing the shell script **'run_train.sh'** in the **'train'** directory.
- You can modify the base model, dataset path, or hyperparameters in the shell script.
```
cd train
sh run_train.sh
```

**(3) Evaluation**
- Evaluate your FinCSE by executing the shell script **'run_eval.sh'** in the **'evaluation'** directory.
- You can modify the model to be evaluated, the dataset path, or the hyperparameters in the shell script.
```
cd evaluation
sh run_eval.sh
```

**(4) Inference**
- Perform inference by executing the shell script **'run_sentence_retrieval.sh'** in the **'evaluation'** directory.
- Note that the Faiss index and the pickle file of sentences are required to perform inference.
```
cd evaluation
sh run_sentence_retrieval.sh
```

## 4. Checkpoints
- You can use FinCSE models based on mBERT and XLM-RoBERTa by downloading them from **HuggingFace**.
  - [fincse-multilingual-bert-cased](https://huggingface.co/snumin44/fincse-multilingual-bert-cased)  
  - [fincse-multilingual-xlm-roberta-base](fincse-multilingual-xlm-roberta-base)
  - [fincse-multilingual-xlm-roberta-large](https://huggingface.co/snumin44/fincse-multilingual-xlm-roberta-large)

- The performance of these FinCSE models on financial **sentence retrieval** task is as follows. (This performance is evaluated using 50,000 samples from the dataset, which were not used in the fine-tuning.)

||FinCSE-mbert-cased|FinCSE-xlm-roberta-base|FinCSE-xlm-roberta-large|
|:---:|:---:|:---:|:---:|
|**ACC@1**|97.14|97.24|97.75|
|**ACC@5**|98.75|98.76|98.86|
|**ACC@10**|98.87|98.88|98.91|

(â€» It is recommended to use the FinCSE model based on XLM-RoBERTa, as the mBERT-based model is unstable.)


## 5. Examples
- Query : ì¤‘êµ­ ìƒì„±í˜• AI ê¸°ì—… í˜„í™© (= Status of generative AI companies in China)
- Texts :

||Text|Translation|
|:---:|:---:|:---:|
|**EN**|Experts predict that the U.S. Federal Reserve will lower interest rates by 0.25 percentage points this month, predicting that interest rates will fall by more than 0.5 percentage points in total within this year.|Experts predict that the U.S. Federal Reserve will lower interest rates by 0.25 percentage points this month, predicting that interest rates will fall by more than 0.5 percentage points in total within this year.|
|**ZN**|é“çŸ¿çŸ³æ˜¯é«˜åº¦ä¾èµ–ä¸­å›½éœ€æ±‚çš„åŸææ–™ï¼Œä¸­å›½å å„çŸ¿ä¸šå…¬å¸å‡ºå£çš„70%ã€‚|Iron ore is a raw material that is highly dependent on China's demand, with China accounting for 70% of exports from mining companies.|
|**JA**|ã‚ªãƒ©ã‚¯ãƒ«ã¯ä»Šå¹´ã®å¤§å‹æŠ€è¡“æ ªã®ã†ã¡ã€æ ªä¾¡ä¸Šæ˜‡ç‡ãŒäººå·¥çŸ¥èƒ½ï¼ˆï¼¡ï¼©ï¼‰åŠå°ä½“å¤§å°†ä¸»ã§ã‚ã‚‹ï¼®ï¼¶ï¼©ï¼¤ï¼©ï¼¡ã®ï¼‘ï¼“ï¼™ï¼…ä¸Šæ˜‡ç‡ã‚’é™¤ã‘ã°æœ€ã‚‚é«˜ã„ã€‚|Oracle's stock price increase is the second highest among major tech stocks this year, after NVIDIA, the leader in AI semiconductors, with a 139% increase.|
|**VI**|NYT chá»‰ ra ráº±ng Ä‘iá»ƒm yáº¿u cá»§a ngÃ nh cÃ´ng nghiá»‡p Trung Quá»‘c lÃ  LLM vÃ  háº§u háº¿t cÃ¡c chÆ°Æ¡ng trÃ¬nh mÃ  cÃ¡c cÃ´ng ty Trung Quá»‘c Ä‘Æ°a ra dÆ°á»›i dáº¡ng AI tá»•ng há»£p trÃªn thá»±c táº¿ Ä‘á»u Ä‘Æ°á»£c nháº­p kháº©u tá»« Má»¹ vÃ  Ä‘Æ°á»£c cáº£i tiáº¿n.|The New York Times points out that the weakness of China's industry lies in Large Language Models (LLMs), and that most of the programs offered by Chinese companies in the form of generative AI are actually imported from the U.S. and then improved upon.|
|**ID**|Harga saham Trump Media, perusahaan induk Truth Social, perusahaan layanan jejaring sosial (SNS) yang didirikan oleh mantan calon presiden AS dari Partai Republik Donald Trump, anjlok lebih dari 10%.|The stock price of Trump Media, the parent company of Truth Social, the social networking service (SNS) founded by former U.S. presidential candidate from the Republican Party, Donald Trump, plunged by more than 10%.|
  


```python
import numpy as np
from transformers import AutoModel, AutoTokenizer

model_path = 'snumin44/fincse-multilingual-xlm-roberta-base'
model = AutoModel.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)

query = 'ì¤‘êµ­ ìƒì„±í˜• AI ê¸°ì—… í˜„í™©'

targets = [
    "Experts predict that the U.S. Federal Reserve will lower interest rates by 0.25 percentage points this month, predicting that interest rates will fall by more than 0.5 percentage points in total within this year.",
    "é“çŸ¿çŸ³æ˜¯é«˜åº¦ä¾èµ–ä¸­å›½éœ€æ±‚çš„åŸææ–™ï¼Œä¸­å›½å å„çŸ¿ä¸šå…¬å¸å‡ºå£çš„70%ã€‚",
    "ã‚ªãƒ©ã‚¯ãƒ«ã¯ä»Šå¹´ã®å¤§å‹æŠ€è¡“æ ªã®ã†ã¡ã€æ ªä¾¡ä¸Šæ˜‡ç‡ãŒäººå·¥çŸ¥èƒ½ï¼ˆï¼¡ï¼©ï¼‰åŠå°ä½“å¤§å°†ä¸»ã§ã‚ã‚‹ï¼®ï¼¶ï¼©ï¼¤ï¼©ï¼¡ã®ï¼‘ï¼“ï¼™ï¼…ä¸Šæ˜‡ç‡ã‚’é™¤ã‘ã°æœ€ã‚‚é«˜ã„ã€‚",
    "NYT chá»‰ ra ráº±ng Ä‘iá»ƒm yáº¿u cá»§a ngÃ nh cÃ´ng nghiá»‡p Trung Quá»‘c lÃ  LLM vÃ  háº§u háº¿t cÃ¡c chÆ°Æ¡ng trÃ¬nh mÃ  cÃ¡c cÃ´ng ty Trung Quá»‘c Ä‘Æ°a ra dÆ°á»›i dáº¡ng AI tá»•ng há»£p trÃªn thá»±c táº¿ Ä‘á»u Ä‘Æ°á»£c nháº­p kháº©u tá»« Má»¹ vÃ  Ä‘Æ°á»£c cáº£i tiáº¿n.",
    "Harga saham Trump Media, perusahaan induk Truth Social, perusahaan layanan jejaring sosial (SNS) yang didirikan oleh mantan calon presiden AS dari Partai Republik Donald Trump, anjlok lebih dari 10%."
]

query_feature = tokenizer(query, return_tensors='pt')
query_outputs = model(**query_feature, return_dict=True)
query_embeddings = query_outputs.pooler_output.detach().numpy().squeeze()

def cos_sim(A, B):
    return np.dot(A, B) / (np.linalg.norm(A) * np.linalg.norm(B))

for idx, target in enumerate(targets):
    target_feature = tokenizer(target, return_tensors='pt')
    target_outputs = model(**target_feature, return_dict=True)
    target_embeddings = target_outputs.pooler_output.detach().numpy().squeeze()
    similarity = cos_sim(query_embeddings, target_embeddings)
    print(f"Similarity between query and target {idx}: {similarity:.4f}")
```
```

```

## Citing
```

```
